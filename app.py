
# DreamRewrite Pro (LLM) â€” Arabic Streamlit app for Dream Interpretation Articles
# - Rule-based analysis + optional OpenAI LLM analysis
# - Rule-based rewrite + optional OpenAI LLM rewrite
#
# Run locally:
#   pip install -r requirements.txt
#   streamlit run app.py
#
# Deploy on Streamlit Cloud:
#   Push to GitHub, then New app â†’ pick repo â†’ app.py

import re
import json
import textwrap
from datetime import datetime
from collections import Counter, defaultdict

import streamlit as st

# ============ Optional: OpenAI LLM ============
OPENAI_AVAILABLE = True
try:
    from openai import OpenAI
except Exception:
    OPENAI_AVAILABLE = False

# -----------------------
# Helpers (Arabic-aware)
# -----------------------
AR_SENT_SPLIT = re.compile(r"[\.!ØŸ\?\n]+")
AR_WS = re.compile(r"\s+")
AR_PUNCT = re.compile(r"[\,\.;:!ØŸ\-â€”\(\)\[\]\{\}\"\'Â«Â»â€¦]")

def normalize_text(s: str) -> str:
    s = s.replace("\u200f", "").replace("\u200e", "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def split_sentences(s: str):
    s = normalize_text(s)
    parts = [p.strip() for p in AR_SENT_SPLIT.split(s) if p.strip()]
    return parts

def tokenize(s: str):
    s = AR_PUNCT.sub(" ", s)
    toks = [t for t in AR_WS.split(s) if t]
    return toks

# -----------------------
# Analysis (rule-based)
# -----------------------
def readability_metrics(text: str):
    sents = split_sentences(text)
    toks = tokenize(text)
    avg_sent_len = (sum(len(tokenize(s)) for s in sents) / max(1, len(sents))) if sents else 0
    long_sentences = [s for s in sents if len(tokenize(s)) > 30]
    return {
        "sentences": len(sents),
        "tokens": len(toks),
        "avg_sentence_tokens": round(avg_sent_len, 2),
        "long_sentence_count": len(long_sentences),
    }

def repetition_flags(text: str):
    sents = split_sentences(text)
    seen = Counter(sents)
    repeated_sents = [s for s, c in seen.items() if c > 1]
    words = tokenize(text)
    grams = [" ".join(words[i:i+5]) for i in range(max(0, len(words)-4))]
    gcount = Counter(grams)
    repeated_ngrams = [g for g, c in gcount.items() if c > 1 and len(g.split()) == 5]
    return {
        "repeated_sentence_samples": repeated_sents[:3],
        "repeated_5gram_samples": repeated_ngrams[:5],
    }

def heading_structure_flags(text: str):
    lines = [l.strip() for l in text.splitlines()]
    h2 = [l for l in lines if l.startswith("## ") or l.startswith("### ")]
    has_faq_section = any("FAQ" in l or "Ø£Ø³Ø¦Ù„Ø©" in l for l in lines)
    return {
        "heading_lines_found": len(h2),
        "has_faq_section": has_faq_section,
    }

def disclaimer_present(text: str):
    patterns = [r"Ø§Ø¬ØªÙ‡Ø§Ø¯", r"Ù‚Ø¯", r"ÙŠÙØ­ØªÙ…Ù„", r"Ù„ÙŠØ³Øª Ø¨Ø¯ÙŠÙ„Ù‹Ø§", r"ØªÙ†Ø¨ÙŠÙ‡", r"Ø¸Ø±ÙˆÙ", r"Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚"]
    score = sum(bool(re.search(p, text)) for p in patterns)
    return {"has_disclaimer_signals": score >= 2, "signals_matched": score}

def keyword_stats(text: str, primary_kw: str, related_kws: list[str]):
    toks = tokenize(text)
    total = len(toks) or 1
    pk_count = sum(1 for t in toks if primary_kw and primary_kw in t)
    rel_counts = {rk: sum(1 for t in toks if rk and rk in t) for rk in related_kws}
    return {
        "total_tokens": total,
        "primary_kw_count": pk_count,
        "primary_kw_density": round(pk_count / total, 4),
        "related_counts": rel_counts,
    }

def analyze_text_rule(text: str, primary_kw: str, related_kws: list[str]):
    metrics = readability_metrics(text)
    reps = repetition_flags(text)
    heads = heading_structure_flags(text)
    disc = disclaimer_present(text)
    kws = keyword_stats(text, primary_kw, related_kws)

    issues = []
    if metrics["avg_sentence_tokens"] > 25:
        issues.append("Ø¬ÙÙ…Ù„ Ø·ÙˆÙŠÙ„Ø©Ø› ÙŠÙØ¶Ù‘Ù„ ØªÙ‚ØµÙŠØ± Ø§Ù„Ø¬ÙÙ…Ù„ ÙˆØ¬Ø¹Ù„Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©.")
    if metrics["long_sentence_count"] > 0:
        issues.append("Ù‡Ù†Ø§Ùƒ Ø¬ÙÙ…Ù„ Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§Ø› Ù‚Ø³Ù‘Ù…Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ØªÙŠÙ† Ø£Ùˆ Ø«Ù„Ø§Ø«.")
    if reps["repeated_sentence_samples"]:
        issues.append("ØªÙƒØ±Ø§Ø± Ø¬ÙÙ…Ù„ Ø¨Ø¹ÙŠÙ†Ù‡Ø§Ø› Ø£Ø¹Ø¯ Ø§Ù„ØµÙŠØ§ØºØ© ÙˆØªØ®Ù„Ù‘Øµ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±.")
    if reps["repeated_5gram_samples"]:
        issues.append("ØªÙƒØ±Ø§Ø± Ø¹Ø¨Ø§Ø±Ø§Øª (n-grams) ÙŠÙˆØ­ÙŠ Ø¨Ø­Ø´ÙˆØ› Ù‚Ù„Ù‘Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±.")
    if heads["heading_lines_found"] < 2:
        issues.append("Ù‡ÙŠÙƒÙ„ Ø¶Ø¹ÙŠÙØ› Ø£Ø¶Ù Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆØ§Ø¶Ø­Ø© (H2/H3).")
    if not heads["has_faq_section"]:
        issues.append("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙØ­Ø©.")
    if not disc["has_disclaimer_signals"]:
        issues.append("Ø£Ø¶Ù ØªÙ†Ø¨ÙŠÙ‡Ù‹Ø§ Ù…Ø³Ø¤ÙˆÙ„Ù‹Ø§ ÙˆØµÙŠØ§ØºØ© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© (Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª).")
    if kws["primary_kw_density"] > 0.02:
        issues.append("ÙƒØ«Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø±ØªÙØ¹Ø©Ø› Ø®ÙÙ‘Ø¶ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„ØªØ¬Ù†Ù‘Ø¨ Ø§Ù„Ø­Ø´Ùˆ.")

    report = {
        "readability": metrics,
        "repetition": reps,
        "structure": heads,
        "disclaimer": disc,
        "keywords": kws,
        "issues": issues,
        "score_people_first": _score_people_first(metrics, reps, heads, disc),
    }
    return report

def _score_people_first(metrics, reps, heads, disc):
    score = 0
    if metrics["avg_sentence_tokens"] <= 22: score += 6
    if metrics["long_sentence_count"] == 0: score += 2
    if not reps["repeated_sentence_samples"]: score += 4
    if heads["heading_lines_found"] >= 3: score += 4
    if disc["has_disclaimer_signals"]: score += 4
    return min(score, 20)

# -----------------------
# Rewrite (rule-based)
# -----------------------
def top_sentences(text: str, primary_kw: str, related_kws: list[str], k=3):
    sents = split_sentences(text)
    scores = []
    for s in sents:
        sc = 0
        if primary_kw:
            sc += s.count(primary_kw) * 3
        for rk in related_kws:
            sc += s.count(rk)
        sc += min(3, len(tokenize(s)) // 12)
        scores.append((sc, s))
    scores.sort(reverse=True, key=lambda x: x[0])
    picked = [s for _, s in scores[:k] if s]
    return picked

def soften_sentence(s: str):
    if re.search(r"(Ø³ÙŠØ­Ø¯Ø«|Ø£ÙƒÙŠØ¯|Ø­ØªÙ…Ø§Ù‹|Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©)", s):
        s = re.sub(r"(Ø³ÙŠØ­Ø¯Ø«|Ø£ÙƒÙŠØ¯|Ø­ØªÙ…Ø§Ù‹|Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©)", "Ù‚Ø¯", s)
    if not re.search(r"Ù‚Ø¯|ÙŠÙØ­ØªÙ…Ù„|Ø±Ø¨Ù…Ø§|Ø­Ø³Ø¨", s):
        s = "Ù‚Ø¯ " + s
    return s

def build_tldr(orig_text: str, primary_kw: str, related_kws: list[str]):
    picks = top_sentences(orig_text, primary_kw, related_kws, k=2)
    if not picks:
        picks = [f"{primary_kw} ÙÙŠ Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ù‚Ø¯ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù†Ù Ù…Ø®ØªÙ„ÙØ© Ø­Ø³Ø¨ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø±Ø§Ø¦ÙŠ."] if primary_kw else ["Ø§Ù„Ø£Ø­Ù„Ø§Ù… ØªÙÙØ³Ù‘ÙØ± ÙˆÙÙ‚ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø´Ø®ØµÙŠ."]
    picks = [soften_sentence(normalize_text(p)) for p in picks]
    tldr = "\n".join([f"- {p.strip()}" for p in picks])
    if primary_kw:
        tldr += f"\n- ÙŠØ®ØªÙ„Ù Ù…Ø¹Ù†Ù‰ {primary_kw} Ø¨Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù…ÙƒØ§Ù†ØŒ ÙˆØ§Ù„ØªÙØ³ÙŠØ± Ø§Ø¬ØªÙ‡Ø§Ø¯ÙŠ."
    return tldr

def plan_related_distribution(related_kws: list[str]):
    buckets = [
        "## Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ",
        "## Ø­Ø³Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¦ÙŠ/Ø§Ù„Ø±Ø§Ø¦ÙŠØ©",
        "## Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù„Ù…Ø´Ø§Ø¹Ø±/Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„ÙØ¹Ù„)",
        "## Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØ¹Ù†ÙŠÙ‡ Ù‡Ø°Ø§ Ø§Ù„Ø­Ù„Ù…ØŸ",
        "## Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ù„Ù‚",
        "## Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© (FAQ)",
    ]
    plan = defaultdict(list)
    for i, rk in enumerate(related_kws):
        plan[buckets[i % len(buckets)]].append(rk)
    return plan

def rewrite_article_rule(orig_text: str, primary_kw: str, related_kws: list[str]):
    primary_kw = (primary_kw or "").strip()
    related_kws = [rk.strip() for rk in (related_kws or []) if rk.strip()]
    now = datetime.now().date().isoformat()
    rel_plan = plan_related_distribution(related_kws)
    tldr = build_tldr(orig_text or "", primary_kw, related_kws)

    h1 = f"ØªÙØ³ÙŠØ± {primary_kw} ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù…: Ø§Ù„Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø£Ù‡Ù… ÙˆÙƒÙŠÙ ÙŠØ®ØªÙ„Ù Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚" if primary_kw else "ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…: Ø§Ù„Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø£Ù‡Ù… ÙˆÙƒÙŠÙ ÙŠØ®ØªÙ„Ù Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚"

    parts = []
    parts.append(f"# {h1}")
    if primary_kw:
        parts.append(f"**TL;DR:**\n{tldr}")

    rel_here = ", ".join(rel_plan["## Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ"]) if rel_plan else ""
    parts.append("## Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ")
    p1 = (
        f"{primary_kw} Ù‚Ø¯ ÙŠØ¹Ø¨Ù‘Ø± Ø¹Ù† Ø¯Ù„Ø§Ù„Ø§ØªÙ Ù…Ø®ØªÙ„ÙØ© ØªØ¨Ø¹Ù‹Ø§ Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø´Ø®ØµÙŠ Ù„Ù„Ø±Ø§Ø¦ÙŠØŒ Ù…Ø«Ù„ Ø¸Ø±ÙˆÙÙ‡ Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆÙ…Ø´Ø§Ø¹Ø±Ù‡ ÙˆÙ‚Øª Ø§Ù„Ø­Ù„Ù…. "
        f"ÙŠÙØ­ØªÙ…Ù„ Ø£Ù† ØªØ±ØªØ¨Ø· Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø¨Ø±ØºØ¨Ø§Øª Ø£Ùˆ Ù…Ø®Ø§ÙˆÙØŒ ÙˆÙ„Ø§ ÙŠÙÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ Ù†ØªÙŠØ¬Ø© Ø­ØªÙ…ÙŠØ©."
    ) if primary_kw else (
        "Ù‚Ø¯ ØªØ¹ÙƒØ³ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ø±ØºØ¨Ø§Øª Ø£Ùˆ Ù…Ø®Ø§ÙˆÙ Ø£Ùˆ Ø£Ø­Ø¯Ø§Ø«Ù‹Ø§ Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„ÙˆØ¹ÙŠØŒ ÙˆØªØªØºÙŠÙ‘Ø± Ø¯Ù„Ø§Ù„ØªÙ‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø®Øµ ÙˆØ§Ù„Ø³ÙŠØ§Ù‚."
    )
    if rel_here:
        p1 += f" ÙƒÙ…Ø§ Ù‚Ø¯ ØªØªÙ‚Ø§Ø·Ø¹ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø«Ù„: {rel_here}."
    parts.append(p1)

    rel_here = ", ".join(rel_plan["## Ø­Ø³Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¦ÙŠ/Ø§Ù„Ø±Ø§Ø¦ÙŠØ©"]) if rel_plan else ""
    parts.append("## Ø­Ø³Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¦ÙŠ/Ø§Ù„Ø±Ø§Ø¦ÙŠØ©")
    parts.append(
        textwrap.dedent(
            f"""
            - **Ø§Ù„Ø¹Ø²Ø¨Ø§Ø¡:** Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¸Ù‡ÙˆØ± {primary_kw or "Ø§Ù„Ø±Ù…Ø²"} Ø¥Ù„Ù‰ Ø¯Ù„Ø§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø£Ùˆ Ø§Ù„ØªØ±Ù‚Ù‘Ø¨ØŒ ÙˆÙŠØ®ØªÙ„Ù Ø¨Ø­Ø³Ø¨ Ù…Ø´Ø§Ø¹Ø±Ù‡Ø§ ÙˆØ¸Ø±ÙˆÙÙ‡Ø§.
            - **Ø§Ù„Ù…ØªØ²ÙˆØ¬Ø©:** ÙŠÙØ­ØªÙ…Ù„ Ø£Ù† ÙŠØ±ØªØ¨Ø· Ø¨Ø§Ù„Ø³Ø¹ÙŠ Ø¥Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙˆØ§Ù„Ø£Ù…Ø§Ù†ØŒ Ø£Ùˆ Ø¨Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª.
            - **Ø§Ù„Ø­Ø§Ù…Ù„:** Ù‚Ø¯ ÙŠØªØµÙ„ Ø¨Ø§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø© Ø£Ùˆ Ø§Ù„Ù‚Ù„Ù‚ Ù…Ù† Ø§Ù„ØªØºÙŠÙ‘Ø±ØŒ ÙˆØªÙØ³ÙŠØ±Ù‡ Ù…ØªØ¨Ø¯Ù‘Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¹ÙˆØ± Ø§Ù„Ù…Ø±Ø§ÙÙ‚.
            - **Ø§Ù„Ø±Ø¬Ù„:** ÙŠÙØ­ØªÙ…Ù„ Ø£Ù† ÙŠØ¹ÙƒØ³ Ø£Ù‡Ø¯Ø§ÙÙ‹Ø§ Ø¹Ù…Ù„ÙŠØ© Ø£Ùˆ Ù…Ø®Ø§ÙˆÙÙ‹Ø§ Ø¸Ø±ÙÙŠØ©.
            """
        ).strip()
    )
    if rel_here:
        parts.append(f"(Ø±Ù…ÙˆØ² Ù…Ø±ØªØ¨Ø·Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…: {rel_here}).")

    rel_here = ", ".join(rel_plan["## Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù„Ù…Ø´Ø§Ø¹Ø±/Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„ÙØ¹Ù„)"]) if rel_plan else ""
    parts.append("## Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù„Ù…Ø´Ø§Ø¹Ø±/Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„ÙØ¹Ù„)")
    parts.append(
        textwrap.dedent(
            f"""
            - **Ù…Ø´Ø§Ø¹Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©:** Ù‚Ø¯ ØªØ¯Ù„ Ø¹Ù„Ù‰ ÙØ±ØµÙ Ø£Ùˆ Ø·Ù…Ø£Ù†ÙŠÙ†Ø© Ù…Ø¤Ù‚ØªØ©.
            - **Ù…Ø´Ø§Ø¹Ø± Ø³Ù„Ø¨ÙŠØ©:** ÙŠÙØ­ØªÙ…Ù„ Ø£Ù† ØªØ¹ÙƒØ³ Ù‚Ù„Ù‚Ù‹Ø§ ÙŠØ­ØªØ§Ø¬ ÙÙ‡Ù… Ø£Ø³Ø¨Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©.
            - **ÙÙŠ Ø§Ù„Ø¨ÙŠØª/Ø§Ù„Ø¹Ù…Ù„:** ÙŠØªØ¨Ø¯Ù‘Ù„ Ø§Ù„Ù…Ø¹Ù†Ù‰ ØªØ¨Ø¹Ù‹Ø§ Ù„Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ø±ØªØ¨Ø§Ø·Ø§ØªÙ‡ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.
            - **ÙŠØ¹Ø·ÙŠ/ÙŠØ£Ø®Ø°:** Ù‚Ø¯ ÙŠØªØºÙŠÙ‘Ø± Ø§Ù„ØªØ£ÙˆÙŠÙ„ ÙˆÙÙ‚ Ø§Ù„ÙØ¹Ù„ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø·Ø±Ø§Ù.
            """
        ).strip()
    )
    if rel_here:
        parts.append(f"(Ø±Ù…ÙˆØ² Ù…Ø±ØªØ¨Ø·Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…: {rel_here}).")

    rel_here = ", ".join(rel_plan["## Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØ¹Ù†ÙŠÙ‡ Ù‡Ø°Ø§ Ø§Ù„Ø­Ù„Ù…ØŸ"]) if rel_plan else ""
    parts.append("## Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØ¹Ù†ÙŠÙ‡ Ù‡Ø°Ø§ Ø§Ù„Ø­Ù„Ù…ØŸ")
    parts.append(
        f"Ù„Ø§ ÙŠØ¹Ù†ÙŠ Ø¸Ù‡ÙˆØ± {primary_kw or 'Ø§Ù„Ø±Ù…Ø²'} Ù†ØªØ§Ø¦Ø¬ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø¤ÙƒØ¯Ø©ØŒ ÙˆÙ„Ø§ ÙŠÙØ³ØªØ®Ø¯Ù… ÙƒØ£Ø³Ø§Ø³ Ù„Ù‚Ø±Ø§Ø±Ø§ØªÙ Ù…ØµÙŠØ±ÙŠØ©. "
        f"Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ø­ØªÙ…Ø§Ù„ÙŠ ÙˆÙŠØªØ¨Ø¯Ù‘Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø®Øµ ÙˆØ§Ù„Ø¸Ø±ÙˆÙØŒ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ù‡Ù†Ø§ Ù…Ø¹Ø±ÙÙŠØ© Ù„Ø§ Ø¹Ù„Ø§Ø¬ÙŠØ©."
    )
    if rel_here:
        parts.append(f"(Ù„Ù„Ø§Ø·Ù„Ø§Ø¹: Ø±Ù…ÙˆØ² Ù…Ø±ØªØ¨Ø·Ø© Ø´Ø§Ø¦Ø¹Ø©: {rel_here}).")

    rel_here = ", ".join(rel_plan["## Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ù„Ù‚"]) if rel_plan else ""
    parts.append("## Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ù„Ù‚")
    parts.append(
        "- Ø¯ÙˆÙÙ‘Ù† Ø§Ù„Ø­Ù„Ù… ÙˆÙ…Ø´Ø§Ø¹Ø±Ùƒ ÙÙˆØ± Ø§Ù„Ø§Ø³ØªÙŠÙ‚Ø§Ø¸.\n"
        "- Ø§Ø±Ø¨Ø· Ø¨ÙŠÙ† ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø­Ù„Ù… ÙˆØ¸Ø±ÙˆÙÙƒ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©.\n"
        "- Ø®ÙÙ‘Ù Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ø­Ø±ÙÙŠØŒ ÙˆÙÙƒÙ‘Ø± ÙÙŠ Ø§Ù„Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø±Ù…Ø²ÙŠØ©.\n"
        "- Ø¥Ù† Ø§Ø³ØªÙ…Ø± Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ¸Ù‡Ø± Ø£Ø«Ø±Ù‡ Ø¹Ù„Ù‰ ÙŠÙˆÙ…ÙƒØŒ Ø§Ø³ØªØ´Ø± Ù…Ø®ØªØµÙ‹Ø§ Ù†ÙØ³ÙŠÙ‹Ø§ Ù…Ø¤Ù‡Ù„Ù‹Ø§."
    )
    if rel_here:
        parts.append(f"(Ù‚Ø¯ ØªÙÙÙŠØ¯Ùƒ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø±Ù…ÙˆØ²: {rel_here}).")

    rel_here = ", ".join(rel_plan["## Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© (FAQ)"]) if rel_plan else ""
    parts.append("## Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© (FAQ)")
    faq = [
        f"**Ù‡Ù„ ÙŠØ®ØªÙ„Ù ØªÙØ³ÙŠØ± {primary_kw or 'Ø§Ù„Ø±Ù…Ø²'} Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©ØŸ** â€” Ù†Ø¹Ù…ØŒ ÙŠØ®ØªÙ„Ù ØªØ¨Ø¹Ù‹Ø§ Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¦ÙŠ/Ø§Ù„Ø±Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø³ÙŠØ§Ù‚.",
        f"**Ù‡Ù„ Ù„ÙˆÙ†/Ù…ÙƒØ§Ù† Ø¸Ù‡ÙˆØ± {primary_kw or 'Ø§Ù„Ø±Ù…Ø²'} ÙŠØºÙŠÙ‘Ø± Ø§Ù„Ù…Ø¹Ù†Ù‰ØŸ** â€” Ù‚Ø¯ ÙŠØ¨Ø¯Ù‘Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© ØªØ¨Ø¹Ù‹Ø§ Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù„ÙˆÙ†/Ø§Ù„Ù…ÙƒØ§Ù† Ø¨Ø®Ø¨Ø±Ø© Ø§Ù„Ø´Ø®Øµ.",
    ]
    parts.append("\n\n".join(faq))
    if rel_here:
        parts.append(f"(Ø£Ø³Ø¦Ù„Ø© Ù…Ø±ØªØ¨Ø·Ø©: {rel_here}).")

    parts.append("## Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹")
    parts.append("- [Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø§Ù„ÙƒØªØ¨/Ø§Ù„Ø·Ø¨Ø¹Ø§Øª/Ø§Ù„ØµÙØ­Ø§Øª Ø¥Ù† ÙˆÙØ¬Ø¯Øª]\n- [Ù…ÙŠÙ‘Ø² Ø±Ø£ÙŠ Ø§Ù„Ù…Ø­Ø±Ø± Ø¹Ù† Ø§Ù„Ù†Ù‚Ù„]")

    parts.append("\n---\n**Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø«Ù‚Ø©:** Ø§Ù„ÙƒØ§ØªØ¨ â€¢ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ (reviewedBy) â€¢ Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: " + datetime.now().date().isoformat() + " â€¢ Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙØ³ÙŠØ± â€¢ Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªØµØ­ÙŠØ­")

    parts.append(
        "> **ØªÙ†Ø¨ÙŠÙ‡:** ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ù…Ø¬Ø§Ù„ Ø§Ø¬ØªÙ‡Ø§Ø¯ÙŠØŒ ØªØªØºÙŠÙ‘Ø± Ø¯Ù„Ø§Ù„Ø§ØªÙ‡ Ø¨Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø´Ø®Øµ ÙˆØ§Ù„Ø¸Ø±ÙˆÙ."
        " Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‡Ù†Ø§ Ù„Ø£ØºØ±Ø§Ø¶ Ù…Ø¹Ø±ÙÙŠØ© ÙˆÙ„ÙŠØ³Øª Ø¨Ø¯ÙŠÙ„Ù‹Ø§ Ø¹Ù† Ù†ØµÙŠØ­Ø© Ø¯ÙŠÙ†ÙŠØ© Ø£Ùˆ Ù†ÙØ³ÙŠØ© Ø£Ùˆ Ø·Ø¨ÙŠØ©."
    )

    doc = "\n\n".join(parts)

    stats = keyword_stats(doc, primary_kw, related_kws)
    if stats["primary_kw_density"] > 0.015:
        doc = doc.replace(primary_kw, "Ù‡Ø°Ø§ Ø§Ù„Ø±Ù…Ø²", max(0, int(stats["primary_kw_count"] * 0.25)))

    return doc

# -----------------------
# OpenAI LLM helpers
# -----------------------
def get_openai_client(provided_key: str | None):
    if not OPENAI_AVAILABLE:
        raise RuntimeError("Ø­Ø²Ù…Ø© openai ØºÙŠØ± Ù…Ø«Ø¨Ù‘ØªØ©. ØªØ­Ù‚Ù‘Ù‚ Ù…Ù† requirements.txt")
    api_key = provided_key or st.secrets.get("OPENAI_API_KEY", "")
    if not api_key:
        raise RuntimeError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ OPENAI_API_KEY ÙÙŠ Ø§Ù„Ø£Ø³Ø±Ø§Ø± Ø£Ùˆ Ø§Ù„Ø­Ù‚Ù„.")
    client = OpenAI(api_key=api_key)
    return client

LLM_GUIDELINES = """
Ø£Ù†Øª Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ÙŠÙ„ØªØ²Ù… Ø¨Ù€ People-first + E-E-A-T Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù….
Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨ØªÙ‡ ÙˆÙÙ‚ Ø§Ù„Ù†Ù‚Ø§Ø·:
- Ù…Ù‚Ø¯Ù…Ø© TL;DR (3â€“4 Ø£Ø³Ø·Ø±) ØªØ´Ø±Ø­ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£Ø´ÙŠØ¹ + Ù…ØªÙ‰ ÙŠØ®ØªÙ„Ù + ØªÙ†Ø¨ÙŠÙ‡ Ù„Ù„Ø³ÙŠØ§Ù‚.
- ØªØºØ·ÙŠØ© Ø§Ù„Ø­Ø§Ù„Ø§Øª: (Ø§Ù„Ø¹Ø²Ø¨Ø§Ø¡/Ø§Ù„Ù…ØªØ²ÙˆØ¬Ø©/Ø§Ù„Ø­Ø§Ù…Ù„/Ø§Ù„Ø±Ø¬Ù„).
- Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚: (Ù…Ø´Ø§Ø¹Ø±ØŒ Ù…ÙƒØ§Ù†ØŒ ÙØ¹Ù„).
- Ù‚Ø³Ù… "Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØ¹Ù†ÙŠÙ‡ Ø§Ù„Ø­Ù„Ù…ØŸ" Ù„ØªØ¨Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹ØªÙ‚Ø¯Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©.
- Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ù„Ù‚ + Ù…ØªÙ‰ Ø£Ø³ØªØ´ÙŠØ± Ù…Ø®ØªØµÙ‹Ø§.
- FAQ Ø¯Ø§Ø®Ù„ÙŠ (Ø£Ø³Ø¦Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø±Ø¦).
- Ù…Ø±Ø§Ø¬Ø¹ (Ø¥Ù† ÙˆÙØ¬Ø¯Øª) + ØªÙ…ÙŠÙŠØ² Ø±Ø£ÙŠ Ø§Ù„Ù…Ø­Ø±Ø± Ø¹Ù† Ø§Ù„Ù†Ù‚Ù„.
- Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø«Ù‚Ø© (Ø§Ù„ÙƒØ§ØªØ¨/Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹/Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«/Ù…Ù†Ù‡Ø¬ÙŠØ©/Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªØµØ­ÙŠØ­) + ØªÙ†Ø¨ÙŠÙ‡ Ù…Ø³Ø¤ÙˆÙ„.
- ØµÙŠØ§ØºØ© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© (Ù‚Ø¯/ÙŠÙØ­ØªÙ…Ù„/Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚)ØŒ ÙˆØ§Ø­ØªØ±Ø§Ù… Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø§ÙÙŠØ©.
- Ù„Ø§ Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª Ù‚Ø§Ø·Ø¹Ø© Ø£Ùˆ ØªØ´Ø®ÙŠØµ Ø£Ùˆ ØªÙ†Ø¨Ø¤Ø§Øª.
- ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø·Ø¨ÙŠØ¹ÙŠÙ‹Ø§ (Ø¯ÙˆÙ† Ø­Ø´Ùˆ)ØŒ ÙˆØ§Ø³ØªÙ‡Ø¯Ù ÙƒØ«Ø§ÙØ© ØªÙ‚Ø±ÙŠØ¨ÙŠØ© 0.8%â€“1.5% Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ÙˆØ§Ø°ÙƒØ± ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙÙŠ Ù‚Ø³Ù… Ù…Ù†Ø§Ø³Ø¨.
- Ø¹Ù†Ø§ÙˆÙŠÙ† H2/H3 ÙˆØ§Ø¶Ø­Ø©ØŒ ÙˆØ¬ÙÙ…Ù„ Ù‚ØµÙŠØ±Ø© Ù…ÙÙ‡ÙˆÙ…Ø©.
- Ù„Ø§ ØªÙÙ†Ø´Ø¦ Ù…Ø±Ø§Ø¬Ø¹ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©Ø› Ø§ØªØ±Ùƒ Ù…ÙƒØ§Ù†Ù‹Ø§ Ù„Ù„Ù…ØµØ¯Ø± Ø¥Ù† Ù„Ù… ÙŠØªÙˆÙØ±.
Ø£Ù†ØªØ¬ Ø§Ù„Ù†Ø§ØªØ¬ Ø¨ØªØ±Ù…ÙŠØ² Markdown ÙÙ‚Ø·.
"""

def llm_analyze_text(client, model: str, orig: str, primary_kw: str, related_kws: list[str], temperature: float = 0.2):
    prompt = f"""
Ø­Ù„Ù‘Ù„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆÙÙ‚ People-first/E-E-A-TØŒ ÙˆØ§Ø±ØµØ¯ Ø£ÙƒØ¨Ø± 10 Ù…Ø´ÙƒÙ„Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¥ØµÙ„Ø§Ø­ ÙÙˆØ±Ù‹Ø§.
Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒÙ‚Ø§Ø¦Ù…Ø© Ø¹Ù†Ø§ØµØ± Ù…Ø±Ù‚Ù‘Ù…Ø© (+ Ø§Ù‚ØªØ±Ø§Ø­ Ø¥ØµÙ„Ø§Ø­ Ù„ÙƒÙ„ Ù…Ø´ÙƒÙ„Ø©).
Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {primary_kw}
Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©: {", ".join(related_kws)}
Ø§Ù„Ù†Øµ:
\"\"\"
{orig}
\"\"\"
"""
    try:
        # Prefer Responses API
        resp = client.responses.create(
            model=model,
            input=prompt,
            temperature=temperature,
            max_output_tokens=1200,
        )
        return resp.output_text
    except Exception:
        # Fallback: Chat Completions API
        resp = client.chat.completions.create(
            model=model,
            temperature=temperature,
            messages=[
                {"role": "system", "content": "Ø®Ø¨ÙŠØ± ØªØ­Ø±ÙŠØ± Ø¹Ø±Ø¨ÙŠ Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…."},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content

def llm_rewrite(client, model: str, orig: str, primary_kw: str, related_kws: list[str], temperature: float = 0.3):
    prompt = f"""{LLM_GUIDELINES}

Ø£Ø¹Ø¯ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø­ÙŠØ« ÙŠÙ„ØªØ²Ù… Ø¨ÙƒÙ„ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø£Ø¹Ù„Ø§Ù‡.
- Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {primary_kw}
- Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©: {", ".join(related_kws)}
- Ø£Ø¯Ø±Ø¬ TL;DR ÙÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø«Ù… Ø¨Ù‚ÙŠØ© Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚ØªØ±Ø­.
- Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø¯ÙˆÙ† Ø­Ø´Ùˆ.
- Ø£Ø®ØªÙ… Ø¨ØªÙ†Ø¨ÙŠÙ‡ Ù…Ø³Ø¤ÙˆÙ„.
Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:
\"\"\"
{orig}
\"\"\"
"""
    try:
        resp = client.responses.create(
            model=model,
            input=prompt,
            temperature=temperature,
            max_output_tokens=2200,
        )
        return resp.output_text
    except Exception:
        resp = client.chat.completions.create(
            model=model,
            temperature=temperature,
            messages=[
                {"role": "system", "content": "Ù…Ø­Ø±Ø± Ø¹Ø±Ø¨ÙŠ ÙŠÙƒØªØ¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ø¶Ø­ ÙˆÙŠØ±Ø§Ø¹ÙŠ E-E-A-T ÙˆØ³Ù„Ø§Ù…Ø© Ø§Ù„Ø§Ø¯Ø¹Ø§Ø¡Ø§Øª."},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content

# -----------------------
# JSON-LD generator
# -----------------------
def build_jsonld(url: str, site_name: str, primary_kw: str, author: str, reviewer: str, image_url: str = ""):
    today = datetime.now().date().isoformat()
    data = {
        "@context": "https://schema.org",
        "@type": "Article",
        "inLanguage": "ar",
        "headline": f"ØªÙØ³ÙŠØ± {primary_kw} ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù…: Ø§Ù„Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø£Ù‡Ù… ÙˆÙƒÙŠÙ ÙŠØ®ØªÙ„Ù Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚" if primary_kw else "ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…",
        "mainEntityOfPage": {"@type": "WebPage", "@id": url or ""},
        "author": {"@type": "Person", "name": author or ""},
        "reviewedBy": {"@type": "Person", "name": reviewer or ""},
        "publisher": {"@type": "Organization", "name": site_name or ""},
        "datePublished": today,
        "dateModified": today,
        "image": [image_url] if image_url else [],
        "articleSection": ["ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…"],
        "about": [{"@type": "Thing", "name": primary_kw}] if primary_kw else [],
    }
    return json.dumps(data, ensure_ascii=False, indent=2)

# -----------------------
# UI
# -----------------------
st.set_page_config(page_title="DreamRewrite Pro (LLM) â€” Ù…ÙØ­Ù„Ù‘Ù„ ÙˆÙ…ÙØ¹ÙŠØ¯ ÙƒØªØ§Ø¨Ø©", page_icon="ğŸ’¤", layout="wide")
st.title("ğŸ› ï¸ DreamRewrite Pro (LLM) â€” Ù…ÙØ­Ù„Ù‘Ù„ ÙˆÙ…ÙØ¹ÙŠØ¯ ÙƒØªØ§Ø¨Ø© Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…")
st.caption("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ + Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© â†’ ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ + Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ People-first ÙˆE-E-A-T (Ù…Ø¹ Ø®ÙŠØ§Ø± LLM).")

with st.sidebar:
    st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    primary_kw = st.text_input("Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©", value="Ø§Ù„Ù…Ø§Ù„ ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù…")
    related_raw = st.text_area("ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© (Ø³Ø·Ø± Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©)", value="Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ù…Ø§Ù„\nØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø§Ù„\nØ§Ù„Ø°Ù‡Ø¨\nØ§Ù„Ø¯Ù‘ÙÙŠÙ†")
    related_kws = [r.strip() for r in related_raw.splitlines() if r.strip()]
    url = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ù€ JSON-LD)", value="")
    site_name = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Ù„Ù€ JSON-LD)", value="")
    author = st.text_input("Ø§Ø³Ù… Ø§Ù„ÙƒØ§ØªØ¨", value="")
    reviewer = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ (reviewedBy)", value="")
    image_url = st.text_input("Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", value="")

    st.markdown("---")
    st.subheader("ğŸ§  Ø·Ø¨Ù‚Ø© LLM (OpenAI)")
    llm_enabled = st.checkbox("ØªÙØ¹ÙŠÙ„ LLM", value=False)
    default_model = "gpt-4o-mini"
    model = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„", value=default_model, help="Ù…Ø«Ø§Ù„: gpt-4o-mini Ø£Ùˆ gpt-4.1-mini Ø£Ùˆ gpt-5-mini (Ø¥Ù† ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§)")
    temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
    api_key_ui = st.text_input("OPENAI_API_KEY (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ù„Ù… ØªØ¶Ø¹Ù‡ ÙÙŠ Secrets)", type="password")

col1, col2 = st.columns([1,1])

with col1:
    st.subheader("1) Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ")
    orig = st.text_area("Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§", height=260, placeholder="Ø£Ù„ØµÙ‚ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡ ÙˆØ¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨ØªÙ‡â€¦")
    analyze_clicked = st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ (Rule)")
    analyze_llm_clicked = st.button("ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ (LLM)")

with col2:
    st.subheader("2) Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    if analyze_clicked and orig.strip():
        rep = analyze_text_rule(orig, primary_kw, related_kws)
        st.markdown("### ØªÙ‚Ø±ÙŠØ± Rule-based")
        st.write({
            "Ù‚Ø±Ø§Ø¡Ø©": rep["readability"],
            "Ù‡ÙŠÙƒÙ„": rep["structure"],
            "ØªÙƒØ±Ø§Ø±": rep["repetition"],
            "Ø³Ù„Ø§Ù…Ø©/ØªÙ†Ø¨ÙŠÙ‡": rep["disclaimer"],
            "ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©": rep["keywords"],
            "Ù†Ù‚Ø§Ø· People-first (â†’/20)": rep["score_people_first"],
        })
        if rep["issues"]:
            st.warning("\n".join(f"- {i}" for i in rep["issues"]))
        else:
            st.success("Ù„Ø§ Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆÙ‡Ø±ÙŠØ© Ù…ÙƒØªØ´ÙØ© Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¹Ø¯ÙŠ.")

    if analyze_llm_clicked and orig.strip():
        if not llm_enabled:
            st.error("ÙØ¹Ù‘Ù„ LLM Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ÙˆØ£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ OPENAI_API_KEY.")
        else:
            try:
                client = get_openai_client(api_key_ui)
                diag = llm_analyze_text(client, model, orig, primary_kw, related_kws, temperature=temperature)
                st.markdown("### ØªÙ‚Ø±ÙŠØ± LLM")
                st.write(diag)
            except Exception as e:
                st.error(f"ÙØ´Ù„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM: {e}")

st.markdown("---")

st.subheader("3) Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª")
colx, coly = st.columns([1,1])

with colx:
    if st.button("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© (Rule)"):
        rewritten = rewrite_article_rule(orig or "", primary_kw, related_kws)
        st.session_state["rewritten_doc"] = rewritten
        st.session_state["jsonld"] = build_jsonld(url, site_name, primary_kw, author, reviewer, image_url)

    if st.button("ğŸ§  Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© (LLM)"):
        if not llm_enabled:
            st.error("ÙØ¹Ù‘Ù„ LLM Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ÙˆØ£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ OPENAI_API_KEY.")
        else:
            try:
                client = get_openai_client(api_key_ui)
                rewritten = llm_rewrite(client, model, orig or "", primary_kw, related_kws, temperature=temperature)
                st.session_state["rewritten_doc"] = rewritten
                st.session_state["jsonld"] = build_jsonld(url, site_name, primary_kw, author, reviewer, image_url)
            except Exception as e:
                st.error(f"ÙØ´Ù„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM: {e}")

with coly:
    if "rewritten_doc" in st.session_state:
        st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Øµ ÙƒÙ€ .md", data=st.session_state["rewritten_doc"], file_name="rewritten.md", mime="text/markdown")
        st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ JSON-LD", data=st.session_state["jsonld"], file_name="article.schema.json", mime="application/json")

if "rewritten_doc" in st.session_state:
    st.markdown("### Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Markdown)")
    st.code(st.session_state["rewritten_doc"], language="markdown")
    st.markdown("### JSON-LD")
    st.code(st.session_state["jsonld"], language="json")

st.info(
    "Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø§Ø¹Ø¯ÙŠ (Rule) Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…ÙØ§ØªÙŠØ­ØŒ Ø£Ùˆ ØªÙØ¹ÙŠÙ„ LLM (OpenAI) Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø®ÙŠØµ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©.\n"
    "Ø¶Ø¹ Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Secrets Ø¨Ø§Ø³Ù… OPENAI_API_KEY Ø£Ùˆ Ø§Ù„ØµÙ‚Ù‡ Ù…Ø¤Ù‚ØªÙ‹Ø§ ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ."
)
